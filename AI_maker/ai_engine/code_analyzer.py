import openai
import ast
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class AnalysisType(Enum):
    SYNTAX = "syntax"
    LOGIC = "logic"
    STYLE = "style"
    PERFORMANCE = "performance"
    SECURITY = "security"

@dataclass
class CodeIssue:
    line_number: int
    issue_type: AnalysisType
    description: str
    suggestion: str
    severity: str  # low, medium, high
    confidence: float

class CodeAnalyzer:
    def __init__(self, api_key: str, model: str = "gpt-4"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
    
    def analyze_code(self, code: str, language: str, user_level: str) -> Dict:
        """
        ç»¼åˆåˆ†æä»£ç ï¼Œè¿”å›è¯¦ç»†çš„åˆ†æç»“æœ
        """
        # åŸºç¡€è¯­æ³•æ£€æŸ¥
        syntax_issues = self._check_syntax(code, language)
        
        # AIæ·±åº¦åˆ†æ
        ai_analysis = self._ai_analysis(code, language, user_level)
        
        # æ€§èƒ½åˆ†æ
        performance_issues = self._analyze_performance(code, language)
        
        # å®‰å…¨åˆ†æ
        security_issues = self._analyze_security(code, language)
        
        return {
            "syntax_issues": syntax_issues,
            "ai_analysis": ai_analysis,
            "performance_issues": performance_issues,
            "security_issues": security_issues,
            "overall_score": self._calculate_score(syntax_issues, ai_analysis, performance_issues, security_issues)
        }
    
    def _check_syntax(self, code: str, language: str) -> List[CodeIssue]:
        """åŸºç¡€è¯­æ³•æ£€æŸ¥"""
        issues = []
        
        if language.lower() == "python":
            try:
                ast.parse(code)
            except SyntaxError as e:
                issues.append(CodeIssue(
                    line_number=e.lineno or 0,
                    issue_type=AnalysisType.SYNTAX,
                    description=f"è¯­æ³•é”™è¯¯: {e.msg}",
                    suggestion="è¯·æ£€æŸ¥è¯­æ³•é”™è¯¯å¹¶ä¿®æ­£",
                    severity="high",
                    confidence=1.0
                ))
        
        return issues
    
    def _ai_analysis(self, code: str, language: str, user_level: str) -> Dict:
        """ä½¿ç”¨AIè¿›è¡Œæ·±åº¦ä»£ç åˆ†æ"""
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹{language}ä»£ç ï¼Œç”¨æˆ·æ°´å¹³ä¸º{user_level}ã€‚
        
        è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
        1. ä»£ç é€»è¾‘æ˜¯å¦æ­£ç¡®
        2. ä»£ç é£æ ¼æ˜¯å¦è‰¯å¥½
        3. æ˜¯å¦æœ‰æ”¹è¿›ç©ºé—´
        4. é’ˆå¯¹{user_level}æ°´å¹³çš„å»ºè®®
        
        ä»£ç ï¼š
        {code}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ï¼š
        - logic_issues: é€»è¾‘é—®é¢˜åˆ—è¡¨
        - style_issues: é£æ ¼é—®é¢˜åˆ—è¡¨
        - suggestions: æ”¹è¿›å»ºè®®
        - score: æ€»ä½“è¯„åˆ†(0-100)
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            
            # è§£æAIå“åº”
            ai_response = response.choices[0].message.content
            # è¿™é‡Œéœ€è¦è§£æJSONå“åº”ï¼Œç®€åŒ–å¤„ç†
            return {
                "analysis": ai_response,
                "score": 85  # ç¤ºä¾‹åˆ†æ•°
            }
        except Exception as e:
            return {
                "analysis": f"AIåˆ†æå‡ºé”™: {str(e)}",
                "score": 0
            }
    
    def _analyze_performance(self, code: str, language: str) -> List[CodeIssue]:
        """æ€§èƒ½åˆ†æ"""
        issues = []
        
        # ç®€å•çš„æ€§èƒ½æ£€æŸ¥è§„åˆ™
        if language.lower() == "python":
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— é™å¾ªç¯
            if "while True:" in code and "break" not in code:
                issues.append(CodeIssue(
                    line_number=0,
                    issue_type=AnalysisType.PERFORMANCE,
                    description="æ£€æµ‹åˆ°å¯èƒ½çš„æ— é™å¾ªç¯",
                    suggestion="è¯·ç¡®ä¿å¾ªç¯æœ‰æ­£ç¡®çš„é€€å‡ºæ¡ä»¶",
                    severity="high",
                    confidence=0.8
                ))
            
            # æ£€æŸ¥åˆ—è¡¨æ¨å¯¼å¼ä½¿ç”¨
            if "for" in code and "[" in code and "]" in code:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ£€æŸ¥
                pass
        
        return issues
    
    def _analyze_security(self, code: str, language: str) -> List[CodeIssue]:
        """å®‰å…¨åˆ†æ"""
        issues = []
        
        # æ£€æŸ¥SQLæ³¨å…¥é£é™©
        if "sql" in code.lower() and "input(" in code.lower():
            issues.append(CodeIssue(
                line_number=0,
                issue_type=AnalysisType.SECURITY,
                description="å¯èƒ½å­˜åœ¨SQLæ³¨å…¥é£é™©",
                suggestion="è¯·ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢",
                severity="high",
                confidence=0.7
            ))
        
        # æ£€æŸ¥evalä½¿ç”¨
        if "eval(" in code:
            issues.append(CodeIssue(
                line_number=0,
                issue_type=AnalysisType.SECURITY,
                description="ä½¿ç”¨eval()å‡½æ•°å­˜åœ¨å®‰å…¨é£é™©",
                suggestion="è¯·é¿å…ä½¿ç”¨eval()ï¼Œè€ƒè™‘ä½¿ç”¨ast.literal_eval()",
                severity="high",
                confidence=0.9
            ))
        
        return issues
    
    def _calculate_score(self, syntax_issues: List[CodeIssue], 
                        ai_analysis: Dict, 
                        performance_issues: List[CodeIssue],
                        security_issues: List[CodeIssue]) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        base_score = 100
        
        # è¯­æ³•é”™è¯¯æ‰£åˆ†
        syntax_penalty = len([i for i in syntax_issues if i.severity == "high"]) * 20
        syntax_penalty += len([i for i in syntax_issues if i.severity == "medium"]) * 10
        syntax_penalty += len([i for i in syntax_issues if i.severity == "low"]) * 5
        
        # æ€§èƒ½é—®é¢˜æ‰£åˆ†
        perf_penalty = len([i for i in performance_issues if i.severity == "high"]) * 15
        perf_penalty += len([i for i in performance_issues if i.severity == "medium"]) * 8
        perf_penalty += len([i for i in performance_issues if i.severity == "low"]) * 3
        
        # å®‰å…¨é—®é¢˜æ‰£åˆ†
        sec_penalty = len([i for i in security_issues if i.severity == "high"]) * 25
        sec_penalty += len([i for i in security_issues if i.severity == "medium"]) * 15
        sec_penalty += len([i for i in security_issues if i.severity == "low"]) * 8
        
        final_score = max(0, base_score - syntax_penalty - perf_penalty - sec_penalty)
        
        # ç»“åˆAIåˆ†æåˆ†æ•°
        if "score" in ai_analysis:
            final_score = (final_score + ai_analysis["score"]) / 2
        
        return round(final_score, 1)
    
    def generate_feedback(self, analysis_result: Dict, user_level: str) -> str:
        """ç”Ÿæˆç”¨æˆ·å‹å¥½çš„åé¦ˆ"""
        score = analysis_result.get("overall_score", 0)
        
        if score >= 90:
            feedback = "ğŸ‰ ä¼˜ç§€ï¼ä½ çš„ä»£ç è´¨é‡å¾ˆé«˜ï¼Œç»§ç»­ä¿æŒï¼"
        elif score >= 80:
            feedback = "ğŸ‘ å¾ˆå¥½ï¼ä»£ç è´¨é‡ä¸é”™ï¼Œæœ‰ä¸€äº›å°é—®é¢˜å¯ä»¥æ”¹è¿›ã€‚"
        elif score >= 70:
            feedback = "ğŸ“ ä¸é”™ï¼ä»£ç åŸºæœ¬æ­£ç¡®ï¼Œä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´ã€‚"
        elif score >= 60:
            feedback = "âš ï¸ éœ€è¦æ”¹è¿›ã€‚ä»£ç æœ‰ä¸€äº›é—®é¢˜ï¼Œå»ºè®®ä»”ç»†æ£€æŸ¥ã€‚"
        else:
            feedback = "ğŸ”§ éœ€è¦é‡ç‚¹å…³æ³¨ã€‚ä»£ç å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œå»ºè®®é‡æ–°å®¡è§†ã€‚"
        
        # æ·»åŠ å…·ä½“å»ºè®®
        issues_count = len(analysis_result.get("syntax_issues", []))
        if issues_count > 0:
            feedback += f"\nå‘ç° {issues_count} ä¸ªé—®é¢˜éœ€è¦è§£å†³ã€‚"
        
        return feedback 